# 機械学習におけるGradient boostingの各ライブラリを理解してどれを使うか決めたい
@yhiss
shinjuku もくもくプログラミング #36


## WHO
- 金融機関でデータ分析
- 今日はkaggleとGradient Boosting(と各種ライブラリ)の理解を進めてました(結局見ようとした全部はすすんでない)
  - 参考
    - https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
    - http://learningsys.org/nips17/assets/papers/paper_11.pdf
    - https://www.slideshare.net/tkm2261/nips2017-lightgbm-a-highly-efficient-gradient-boosting-decision-tree
    - https://xgboost.readthedocs.io/en/latest/tutorials/model.html
    - https://zaburo-ch.github.io/post/xgboost/

(**論文記事等をいくつか参照**)



## 勾配ブースティング概要
- ***ブースティング***（[英](https://ja.wikipedia.org/wiki/%E8%8B%B1%E8%AA%9E): Boosting）とは、[教師あり学習](https://ja.wikipedia.org/wiki/%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92)を実行するための[機械学習](https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92)[メタアルゴリズム](https://ja.wikipedia.org/wiki/%E3%83%A1%E3%82%BF%E3%83%92%E3%83%A5%E3%83%BC%E3%83%AA%E3%82%B9%E3%83%86%E3%82%A3%E3%82%AF%E3%82%B9)の一種。ブースティングは、Michael Kearns の提示した「一連の**弱い学習機**をまとめることで**強い学習機**を生成できるか？」という疑問に基づいている[[1]](https://ja.wikipedia.org/wiki/%E3%83%96%E3%83%BC%E3%82%B9%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0#cite_note-Kearns88-1)。[wikipedia]
- 機械学習のアルゴリズム
  - 決定木
    - 勾配ブースティング(Gradient Boosting)
    - Random Forest
  - ニューラルネットワーク
  - ロジスティック回帰とか…
  
- **なぜ勾配ブースティングなのか？**
  - **単純に精度高い(Kaggleでも相当頻繁に使われる)**
  - **主要ライブラリ多い(4種類+最近1?)**
    - sklearn:Gradient Boosting
    - XGBoost
    - LightGBM←今日ここまででタイムアップ(以下2つは別の日にする)
    - CatBoost
    - GBDT-PL
****  - **中身わかりづらい(結局中身よくわからないけど、使っている人多そう)**


## 勾配ブースティングの中身
- 決定木のアンサンブル技法(ブースティング)
- 決定木の例(誰がComputer gameが好きなのか)
![](https://d2mxuefqeaa7sj.cloudfront.net/s_53551D0DB8FA1B402ABD56E7BD4A8296E4B0FA43A12D5338B42C5176A0076D7E_1550901618721_1.png)

- アンサンブルの例(足し合わせ)
![](https://d2mxuefqeaa7sj.cloudfront.net/s_53551D0DB8FA1B402ABD56E7BD4A8296E4B0FA43A12D5338B42C5176A0076D7E_1550901751904_2.png)




## 勾配ブースティングで何を学習したいのか(関数定義)
- 分岐(Splitting Positions)をどこにするのか
- 各セグメントのHeightをどんな値にするか
  上記2つを決める
![](https://d2mxuefqeaa7sj.cloudfront.net/s_53551D0DB8FA1B402ABD56E7BD4A8296E4B0FA43A12D5338B42C5176A0076D7E_1550902539868_3.png)


そこで重要な要素が2点

- Training Loss:各点にどれだけ関数がフィットしているか？
- Regularization: 定義した関数がどれだけ複雑か？出来る限りシンプルな方が良い
  (splitting pointsの数やL2ノルム(height)等)


![左上:学習させたいデータに対して 左下:分岐を間違えロス$$L(f)$$が大きい 右上:分岐作りすぎて複雑すぎるΩ(f)大](https://d2mxuefqeaa7sj.cloudfront.net/s_53551D0DB8FA1B402ABD56E7BD4A8296E4B0FA43A12D5338B42C5176A0076D7E_1550903128405_2.png)

## **→出来るだけシンプルな関数で学習させるべきデータにフィットさせるためにガチャガチャする**
# $$Obj =\sum _{i=1}^{n}l(y_{i},\hat{y_{i}}) + \sum_{k=1}^{K}\Omega (f_{k})+constant$$これのconstant以外を最小化する
- 一般的な勾配ブースティングではlossは**二乗誤差**$$l(y_{i},\hat{y_{i}})=\left ( y_{i} - \hat{y_{i}} \right )^{2}$$

$$\hat{y_{i}}^{(0)}=0$$
$$\hat{y_{i}}^{(1)}=f_{1}(x_{i})=\hat{y_{i}}^{(0)}+f_{1}(x_{i})$$
$$\hat{y_{i}}^{(2)}=f_{1}(x_{i})+f_{2}(x_{i})=\hat{y_{i}}^{(1)}+f_{2}(x_{i})$$
…
$$\hat{y_{i}}^{(t)} =\sum_{k=1}^{t}f_{k}(x_{i})=\hat{y_{i}}^{(t-1)}+f_{t}(x_{i})$$

$$\hat{y_{i}}^{(t)}$$:Model at training round t
$$\hat{y_{i}}^{(t-1)}$$:Keep functions added in previos round
$$f_{t}(x_{i})$$:New function($$f_{t}(x_{i})=-[\frac{\partial L(y_{i},f_{t-1}(x_{i}))}{\partial f_{t-1}(x_{i})}]$$)
ちなみにΩは

## ライブラリ毎に
- 分岐点(splitting point)の決め方
- 上記のデータが大きい時の処理
- 目的の関数を最小化させる手法
- 正則化の手法
- カテゴリ変数の処理
- 欠損値の処理
  が異なる
# ライブラリ毎の違い
## 一般的なGradient Boosting(sklearn)
- 最小化の手法:Steepest Descent(再急降下法)
- 分岐の決め方:Pre-Sorted Algorithm
  (分岐になりうる店を列挙して分岐ごとに探査:正確だけど思い)

**→random forest等他の手法よりもわりかし精度高め**


## XGBoost
- 最小化の手法:Newton法
  (目的関数をテイラー展開で二次の項まで近似して、最小化を実施)
- 分岐の決め方:Pre-Sorted Algorithm(デフォルト)
  オプション Histogram-based Algorithm
  (ヒストグラムを作ってそのビンが分岐単位となる:早いが損失対象とは限らない)
- 木の正則化を行う項が目的関数に直接入っている(下図)
- 
- 欠損値:加工せず入れても処理してくれる

**→sklearnより早くてoverfitしにくい。そして欠損値も処理してくれる**
→よってすごい

![](https://d2mxuefqeaa7sj.cloudfront.net/s_53551D0DB8FA1B402ABD56E7BD4A8296E4B0FA43A12D5338B42C5176A0076D7E_1550905678007_5.png)



## LightGBM
- 分岐の決め方:Histogram-based Algorithm
- **正則化等はXGBoostと同じで、おおまかにXGBoostの高速化バージョン(以下が高速化の要因)**
- GOSS(Gradient-based One-side Sampling)
  - 使用するデータを減らす
    - 十分勾配の小さいデータは無視
    - データの分布がおかしくならないようにサンプリングした分を割り戻して使用
    - 因みにデフォルトではOFF
- EFB(Exclusive Feature Bunding)
  - 特徴量をまとめて計算して高速化する
  - 大きなデータでは疎な変数があることも多いため、0でない要素が被らないものも多い→被らないものは纏めることにより、計算量を削減
- 欠損値:加工せず入れても処理してくれる
- カテゴリ変数:加工せず入れても処理してくれる

**→速度、利便性、精度共良いので、上2つよりも使えそう**
**→もっとすごい**


## 結局、最近のkaggleの流れと同じようにLightGBMが良いという結論になる。あとはCatboostとかGBDT-PLの文献漁ります

