# CatBoost: unbiased boosting with categorical features
@yhiss
shinjuku もくもくプログラミング #37


## WHO
- 金融機関でデータ分析
- 今日はkaggleとCatboostの理解を進めてました
  - https://arxiv.org/pdf/1706.09516.pdf
  - https://tech.yandex.com/catboost
  - NeurIPS2018


## 勾配ブースティング概要(ほぼ先週も使った内容)
- ***ブースティング***（[英](https://ja.wikipedia.org/wiki/%E8%8B%B1%E8%AA%9E): Boosting）とは、[教師あり学習](https://ja.wikipedia.org/wiki/%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92)を実行するための[機械学習](https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92)[メタアルゴリズム](https://ja.wikipedia.org/wiki/%E3%83%A1%E3%82%BF%E3%83%92%E3%83%A5%E3%83%BC%E3%83%AA%E3%82%B9%E3%83%86%E3%82%A3%E3%82%AF%E3%82%B9)の一種。ブースティングは、Michael Kearns の提示した「一連の**弱い学習機**をまとめることで**強い学習機**を生成できるか？」という疑問に基づいている[[1]](https://ja.wikipedia.org/wiki/%E3%83%96%E3%83%BC%E3%82%B9%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0#cite_note-Kearns88-1)。[wikipedia]
- 機械学習のアルゴリズム
  - 決定木:分岐を作っていく
    - 勾配ブースティング(Gradient Boosting)
    - Random Forest
  - ニューラルネットワーク
  - ロジスティック回帰とか…


![](https://d2mxuefqeaa7sj.cloudfront.net/s_367A285A0C8E16C6174E8C46C635E73A24CB3CA95B94D34573CAFBBFD14D3D9E_1551508225610_1.png)



- **なぜ勾配ブースティングなのか？**
  - **単純に精度高い(Kaggleでも相当頻繁に使われる)**
  - **主要ライブラリ多い(4種類+最近1?)**
    - sklearn:Gradient Boosting
    - XGBoost:精度高い
    - LightGBM:原理XGBoostに似てる
    - **CatBoost←今日これ**
    - GBDT-PL
  - **中身わかりづらい**
    **結局中身よくわからないけど、使っている人多そう→左記の人たちと差別化を図ろうとする思惑もあり**
## Catboostの特徴
- ***Ordered boosting****という*アルゴリズムを実装
- カテゴリ変数をうまく扱える(らしい)
- *Prediction shift*(過学習の一因の一つ)を防ぐ(らしい)


## 機械学習(テーブルデータ)で前処理が必要な項目
- カテゴリ変数←今日はここにフォーカス
- 欠損値:勝手にいい感じに扱ってくるライブラリも結構ある
- 外れ値
## カテゴリ変数の扱い
- カテゴリ変数とは？
  - ざっくり言ったら都道府県や血液型等の変数(年齢や収入等は一般に数値変数)
- 決定木では分岐を作る。(e.g.年齢が20歳以上か否か)
  しかし、カテゴリ変数そのままでは分岐を作ることができず、数値に変換する必要がある
- encoding手法
  - one-hot:カテゴリ毎に0or1の変数を作る
    - メリット:カテゴリの順番を考慮する必要ない
    - デメリット:次元数が増える
  - Ordinal:通し番号を振る
    - メリット:次元数増えない
    - デメリット:数値の順番が影響する(e.g.B型はA型の2倍なのか？)
  - Target encoding(or Target Statics等):目的変数の割合等で置換する
    - メリット:次元数増えない、値に意味がある
    - デメリット:下手な作り方するとすぐに過学習を起こす
    

元のデータを以下とする

| No | 血液型 | 目的変数(当てるもの) |
| -- | --- | ----------- |
| 1  | A   | 0           |
| 2  | B   | 0           |
| 3  | O   | 1           |
| 4  | A   | 1           |
| 5  | AB  | 1           |

one-hot encoding

| No | 血液型-A | 血液型-B | 血液型-O | 血液型-AB | 目的変数(当てるもの) |
| -- | ----- | ----- | ----- | ------ | ----------- |
| 1  | 1     | 0     | 0     | 0      | 0           |
| 2  | 0     | 1     | 0     | 0      | 0           |
| 3  | 0     | 0     | 1     | 0      | 1           |
| 4  | 1     | 0     | 0     | 0      | 1           |
| 5  | 0     | 0     | 0     | 1      | 1           |

Ordinal encoding

| No | 血液型-ordinal | 目的変数(当てるもの) |
| -- | ----------- | ----------- |
| 1  | 1           | 0           |
| 2  | 2           | 0           |
| 3  | 3           | 1           |
| 4  | 1           | 1           |
| 5  | 4           | 1           |

Target encoding

| No | 血液型-ordinal | 目的変数(当てるもの) |
| -- | ----------- | ----------- |
| 1  | 0.5         | 0           |
| 2  | 0           | 0           |
| 3  | 0.3         | 1           |
| 4  | 0.5         | 1           |
| 5  | 0.8         | 1           |

## Catboostでは
- Target encoding(論文ではTarget statistics)の作り方を工夫
  - Greedy TS
    - 使えるサンプル全部使う
      leakageを起こして汎化性能下がるから✖
  - 他2つ試し…
  - Ordered TS
    - ブートストラップ的にサンプルを取ってきて計算する
    - 勾配ブースティングのステップ毎に違う取り方をしている
    →これを採用している
- オプション的にone-hotもできる
## Prediction shift(数式を書くことをすべて割愛)
- 勾配ブースティングでは各ステップの前ステップの関数の偏微分が含まれた損失関数を最小化する際に、現在のステップの情報を含んでしまうためleak(過学習)しやすい。
- そこでOrdered boostingという手法を使ってアルゴリズムの構造上起こりやすい過学習を起こらないようにした(らしい)
## 他の特徴
- Feature combinations(カテゴリ変数のコンビネーション)もできる(らしい)

→具体的に機能がどうなっているかまだわかってないので、これから調べる

## 論文では、XGBoost、LightGBMより良い結果が出た(と主張している)

結局はデータセット等による気がする…

## まとめ:catboostは
- カテゴリ変数の使い方に注力→下手に手元で加工するより良いと思う
- 他のアルゴリズムとは最適化の仕方が異なる

→カテゴリ変数を含むデータセットに有効では？

